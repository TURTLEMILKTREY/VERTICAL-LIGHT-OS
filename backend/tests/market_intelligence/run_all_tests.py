"""
Comprehensive Test Runner for Market Intelligence Microservices
Runs all individual microservice tests with NO HARDCODED VALUES
Ensures all services are production-ready and safe for users
"""

import unittest
import sys
import os
import argparse
import io
from typing import List, Dict, Any
import json
from datetime import datetime

# Add the backend directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

# Import all test modules
from tests.market_intelligence.test_intelligence_engine import TestIntelligenceEngine, TestIntelligenceEngineIntegration
from tests.market_intelligence.test_competitive_analysis_service import TestCompetitiveAnalysisService, TestCompetitiveAnalysisServiceIntegration
from tests.market_intelligence.test_data_quality_service import TestDataQualityService, TestDataQualityServiceIntegration
from tests.market_intelligence.test_risk_assessment_service import TestRiskAssessmentService, TestRiskAssessmentServiceIntegration
from tests.market_intelligence.test_trend_analysis_service import TestTrendAnalysisService, TestTrendAnalysisServiceIntegration
from tests.market_intelligence.test_market_maturity_service import TestMarketMaturityService, TestMarketMaturityServiceIntegration
from tests.market_intelligence.test_intelligence_orchestrator import TestIntelligenceOrchestrator, TestIntelligenceOrchestratorIntegration


class MarketIntelligenceTestRunner:
    """Comprehensive test runner for all market intelligence microservices"""
    
    def __init__(self):
        self.test_results = {}
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.error_tests = 0
        
    def run_all_tests(self, verbosity: int = 2) -> Dict[str, Any]:
        """Run all microservice tests"""
        print("üöÄ Starting Comprehensive Market Intelligence Microservice Tests")
        print("=" * 80)
        print("Testing ALL 7 microservices with DYNAMIC configuration")
        print("NO HARDCODED VALUES - Safe for user businesses")
        print("=" * 80)
        
        # Define test suites for each microservice
        test_suites = {
            'Intelligence Engine': [TestIntelligenceEngine, TestIntelligenceEngineIntegration],
            'Competitive Analysis Service': [TestCompetitiveAnalysisService, TestCompetitiveAnalysisServiceIntegration],
            'Data Quality Service': [TestDataQualityService, TestDataQualityServiceIntegration],
            'Risk Assessment Service': [TestRiskAssessmentService, TestRiskAssessmentServiceIntegration],
            'Trend Analysis Service': [TestTrendAnalysisService, TestTrendAnalysisServiceIntegration],
            'Market Maturity Service': [TestMarketMaturityService, TestMarketMaturityServiceIntegration],
            'Intelligence Orchestrator': [TestIntelligenceOrchestrator, TestIntelligenceOrchestratorIntegration]
        }
        
        overall_results = {
            'test_run_id': f'test_run_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'timestamp': datetime.now().isoformat(),
            'microservices_tested': len(test_suites),
            'microservice_results': {},
            'overall_summary': {},
            'production_readiness': 'PENDING'
        }
        
        for service_name, test_classes in test_suites.items():
            print(f"\nüîç Testing {service_name}")
            print("-" * 60)
            
            service_results = self._run_service_tests(service_name, test_classes, verbosity)
            overall_results['microservice_results'][service_name] = service_results
            
            # Update counters
            self.total_tests += service_results['total_tests']
            self.passed_tests += service_results['passed_tests']
            self.failed_tests += service_results['failed_tests']
            self.error_tests += service_results['error_tests']
        
        # Generate overall summary
        overall_results['overall_summary'] = {
            'total_tests': self.total_tests,
            'passed_tests': self.passed_tests,
            'failed_tests': self.failed_tests,
            'error_tests': self.error_tests,
            'success_rate': (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0,
            'all_services_passed': self.failed_tests == 0 and self.error_tests == 0
        }
        
        # Determine production readiness
        if overall_results['overall_summary']['all_services_passed']:
            overall_results['production_readiness'] = 'APPROVED ‚úÖ'
        else:
            overall_results['production_readiness'] = 'NEEDS_ATTENTION ‚ùå'
        
        self._print_final_summary(overall_results)
        return overall_results
    
    def _run_service_tests(self, service_name: str, test_classes: List, verbosity: int) -> Dict[str, Any]:
        """Run tests for a specific microservice"""
        service_results = {
            'service_name': service_name,
            'test_classes': len(test_classes),
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'error_tests': 0,
            'test_details': []
        }
        
        for test_class in test_classes:
            # Create test suite
            suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
            
            # Run tests
            stream = io.StringIO()
            runner = unittest.TextTestRunner(stream=stream, verbosity=verbosity)
            result = runner.run(suite)
            
            # Collect results
            class_results = {
                'test_class': test_class.__name__,
                'tests_run': result.testsRun,
                'failures': len(result.failures),
                'errors': len(result.errors),
                'success_rate': ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0,
                'output': stream.getvalue()
            }
            
            service_results['test_details'].append(class_results)
            service_results['total_tests'] += result.testsRun
            service_results['passed_tests'] += (result.testsRun - len(result.failures) - len(result.errors))
            service_results['failed_tests'] += len(result.failures)
            service_results['error_tests'] += len(result.errors)
            
            # Print class results
            if result.failures or result.errors:
                print(f"  ‚ùå {test_class.__name__}: {class_results['success_rate']:.1f}% ({result.testsRun} tests)")
                if verbosity > 1:
                    for failure in result.failures:
                        print(f"    FAIL: {failure[0]}")
                    for error in result.errors:
                        print(f"    ERROR: {error[0]}")
            else:
                print(f"  ‚úÖ {test_class.__name__}: 100% ({result.testsRun} tests)")
        
        # Calculate service success rate
        service_results['success_rate'] = (service_results['passed_tests'] / service_results['total_tests'] * 100) if service_results['total_tests'] > 0 else 0
        
        return service_results
    
    def _print_final_summary(self, results: Dict[str, Any]):
        """Print comprehensive test summary"""
        print("\n" + "=" * 80)
        print("üéØ FINAL TEST RESULTS - MARKET INTELLIGENCE MICROSERVICES")
        print("=" * 80)
        
        summary = results['overall_summary']
        
        print(f"üìä OVERALL STATISTICS:")
        print(f"   Total Tests Run: {summary['total_tests']}")
        print(f"   Passed: {summary['passed_tests']} ‚úÖ")
        print(f"   Failed: {summary['failed_tests']} ‚ùå")
        print(f"   Errors: {summary['error_tests']} ‚ö†Ô∏è")
        print(f"   Success Rate: {summary['success_rate']:.1f}%")
        
        print(f"\nüî¨ MICROSERVICE BREAKDOWN:")
        for service_name, service_results in results['microservice_results'].items():
            status = "‚úÖ" if service_results['failed_tests'] == 0 and service_results['error_tests'] == 0 else "‚ùå"
            print(f"   {status} {service_name}: {service_results['success_rate']:.1f}% ({service_results['total_tests']} tests)")
        
        print(f"\nüöÄ PRODUCTION READINESS: {results['production_readiness']}")
        
        if summary['all_services_passed']:
            print("\nüéâ ALL MICROSERVICES PASSED!")
            print("‚úÖ No hardcoded values detected")
            print("‚úÖ All services are configuration-driven") 
            print("‚úÖ Thread safety verified")
            print("‚úÖ Error handling validated")
            print("‚úÖ Fallback mechanisms confirmed")
            print("‚úÖ SAFE FOR USER BUSINESSES")
            print("\nüü¢ PRODUCTION DEPLOYMENT APPROVED")
        else:
            print("\n‚ö†Ô∏è  SOME TESTS FAILED")
            print("‚ùå Production deployment NOT recommended")
            print("üîß Please fix failing tests before deployment")
        
        print("=" * 80)
    
    def run_specific_service(self, service_name: str, verbosity: int = 2) -> Dict[str, Any]:
        """Run tests for a specific microservice"""
        service_mapping = {
            'intelligence_engine': [TestIntelligenceEngine, TestIntelligenceEngineIntegration],
            'competitive_analysis': [TestCompetitiveAnalysisService, TestCompetitiveAnalysisServiceIntegration],
            'data_quality': [TestDataQualityService, TestDataQualityServiceIntegration],
            'risk_assessment': [TestRiskAssessmentService, TestRiskAssessmentServiceIntegration],
            'trend_analysis': [TestTrendAnalysisService, TestTrendAnalysisServiceIntegration],
            'market_maturity': [TestMarketMaturityService, TestMarketMaturityServiceIntegration],
            'orchestrator': [TestIntelligenceOrchestrator, TestIntelligenceOrchestratorIntegration]
        }
        
        if service_name not in service_mapping:
            print(f"‚ùå Unknown service: {service_name}")
            print(f"Available services: {', '.join(service_mapping.keys())}")
            return {}
        
        test_classes = service_mapping[service_name]
        return self._run_service_tests(service_name, test_classes, verbosity)


def main():
    """Main entry point for test runner"""
    parser = argparse.ArgumentParser(description='Market Intelligence Microservice Test Runner')
    parser.add_argument('--service', '-s', type=str, help='Run tests for specific service only')
    parser.add_argument('--verbosity', '-v', type=int, default=2, help='Test verbosity level (0-2)')
    parser.add_argument('--output', '-o', type=str, help='Save results to JSON file')
    
    args = parser.parse_args()
    
    runner = MarketIntelligenceTestRunner()
    
    if args.service:
        results = runner.run_specific_service(args.service, args.verbosity)
    else:
        results = runner.run_all_tests(args.verbosity)
    
    # Save results if requested
    if args.output and results:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nüìÑ Results saved to: {args.output}")
    
    # Exit with appropriate code
    if results and results.get('overall_summary', {}).get('all_services_passed', False):
        sys.exit(0)  # Success
    else:
        sys.exit(1)  # Failure


if __name__ == '__main__':
    main()
